###
# DATA STORAGE PROCESS
# 1. new data is uploaded through the app 
#   (user uploads files (maybe in a zip folder) or points to a folder on their computer)
# 2. clean the data one file at a time
# 3. save meta data
#     (fileid, filename, site, date min/max, cows, min/max lat/longitude, storage location (rds))
# 4. cleaned data is stored in an rds (e.g., 50 files of data per rds)
# 5. cache.rds keeps track of recent data frames used by the shiny app
# 6. current.df is the current set of data used by the shiny app
#
# functions:
#    clean_batch
#       - clean_df
#       - get_meta
#       - save_meta
#    get_data_from_meta
#       - options to select days/times/cows/sites
#       - check if cached
#    save_to_cache
#   
# the shiny app needs to use some of these functions

#'
#'Generates basic metadata about a directory of animal data files and stores the files as data frames as a list with the meta
#'
#'@param data_dir location of animal data files, in list format
#'@return a list of animal data frames with information about the data
#'
store_batch_list <- function(data_dir) {
  # unpack documents in the .zip file to a temp folder
  dir_name <- gsub(".zip", "", data_dir$name)
  
  unlink(file.path("temp"), recursive=TRUE)
  
  data_files <- unzip(data_dir$datapath, exdir="temp")
  data_files <- list.files("temp", pattern ="*.csv", recursive = TRUE, full.names = T)
  
  rds_name <- paste0(dir_name, ".rds")
  
  data_sets <- lapply(data_files, read.csv, skipNul = T, stringsAsFactors = F)
  
  file_names <- gsub(paste0("(temp)(\\/)", dir_name, "(\\/)"), "", data_files)
  gps_units <- gsub("(.*)(20)([0-9]{2}\\_)(.*)(\\_{1}.*)(\\.csv)","\\4", file_names)
  ani_ids <- gsub("(.*)(20)([0-9]{2}\\_)(.*\\_)(.*)(\\.csv)","\\5", file_names)
  
  site_names <- c()
  
  for(i in 1:length(file_names)) {
    site_names[i] <-  ifelse( grepl("\\_", file_names[i]), tolower(sub("\\_.*","", file_names[i])), paste0("Unknown (", file_names[i], ")"))
  }
  
  ani_ids <- make.unique(ani_ids, sep="_")
  
  unlink("temp", recursive = T)
  
  return(list(data = data_sets, file = file_names, ani = ani_ids, gps = gps_units, site = site_names, rds_name = rds_name))
}

#'
#'Cleans a directory of animal data files
#'
#'@param data_info list of animal data frames with information about the data, generated by store_batch
#'@param autocleans automatically clean data with ts_clean, defaults to true
#'@param filters filter bad data points, defaults to true
#'@return clean df with all animal data files from the directory
#'
clean_batch_df <- function(data_info, autocleans, filters) {
  data_sets <- list()
  withProgress(message = paste0("Preparing raw data", ifelse(filters, " (filtered)", " (unfiltered)")), detail = paste0("0/",length(data_info$data), " files prepped"), value = 0, {
    
  for(i in 1:length(data_info$data)) {
    
    df <- data_info$data[[i]]
    
    suppressWarnings(  df <-  df[!is.na(as.numeric(df$Index)), ] ) # discard any rows with text in the first column duplicate header rows
    df <- type.convert(df)
    
    aniid <- data_info$ani[i]
    gpsid <- data_info$gps[i]
    
    if(data_info$file[i] == aniid) {
      aniid <- paste0("Unknown (", file_names[i], ")")
      data_info$ani[i] <- aniid
    }
    
    if(data_info$file[i] == gpsid) {
      gpsid <- paste0("Unknown (", file_names[i], ")")
      data_info$gps[i] <- gpsid
    }
    
    # clean df
    df_out<- clean_location_data(df, autocleans, filters,
                                 aniid = aniid, 
                                 gpsid = gpsid, 
                                 maxrate = 84, maxcourse = 100, maxdist = 840, maxtime=100, timezone = "UTC")
    # add cleaned df to the list of data
    data_sets[[paste0("ani",aniid)]] <- df_out
    incProgress(1/(2*length(data_info$data)), detail = paste0(i,"/",length(data_info$data), " files prepped"))
  } #cleaning for loop
  })
  return(suppressWarnings(dplyr::bind_rows(data_sets)))
}

#'
#'Cleans a directory of animal data files and stores them locally in rds format
#'
#'@param data_info list of animal data frames with information about the data, generated by store_batch
#'@param autocleans automatically clean data with ts_clean, defaults to true
#'@param filters filter bad data points, defaults to true
#'@param get_slope logical, whether to compute slope (in degrees)
#'@param get_aspect logical, whether to compute aspect (in degrees)
#'@return df of metadata for animal file directory
#'
clean_store_batch <- function(data_info, autocleans, filters, get_slope, get_aspect) {
  
  #initialize empty meta
  meta_df <- data.frame(matrix(ncol = 9, nrow = 0))
  meta_cols <- c("file_id", "file_name", "site", "ani_id", "min_date", "max_date", "min_lat", "max_lat", "storage")
  colnames(meta_df) <- meta_cols
 
  num_saved_rds <- 0
  
  withProgress(message = "Processing data", detail = paste0("0/",length(data_info$data), " files processed"), value = 0, {

  data_sets <- list()
  
  for(i in 1:length(data_info$data)) {
   
    df <- data_info$data[[i]]
    
    suppressWarnings(  df <-  df[!is.na(as.numeric(df$Index)), ] ) # discard any rows with text in the first column duplicate header rows
    df <- type.convert(df)
    
    aniid <- data_info$ani[i]
    gpsid <- data_info$gps[i]
    
    if(data_info$file[i] == aniid) {
      aniid <- paste0("Unknown (", file_names[i], ")")
      data_info$ani[i] <- aniid
    }
    
    if(data_info$file[i] == gpsid) {
      gpsid <- paste0("Unknown (", file_names[i], ")")
      data_info$gps[i] <- gpsid
    }
    
    # clean df
    df_out<- clean_location_data(df, autocleans, filters,
                             aniid = aniid, 
                             gpsid = gpsid, 
                             maxrate = 84, maxcourse = 100, maxdist = 840, maxtime=100, timezone = "UTC")
    # add cleaned df to the list of data
    data_sets[[paste0("ani",aniid)]] <- df_out
    incProgress(1/(2*length(data_info$data)), detail = paste0(i,"/",length(data_info$data), " files cleaned"))
  } #cleaning for loop
    
    all_data_sets <- suppressWarnings(dplyr::bind_rows(data_sets)) 
    
    elev_data_sets <- all_data_sets %>% dplyr::filter(Latitude <= median(Latitude) + 2.5,
                                                      Latitude >= median(Latitude) - 2.5,
                                                      Longitude <= median(Longitude) + 2.5,
                                                      Longitude >= median(Longitude) - 2.5)
   
    
    if(nrow(all_data_sets) != nrow(elev_data_sets)) {
      diff <- nrow(all_data_sets) - nrow(elev_data_sets)
      incProgress(0, detail = paste0("lat/long range greater than 5 degrees. detected. ", diff, " rows filtered. Fetching elevation (this may take a few minutes)..."))
    }
    else {
      incProgress(0, detail = "Fetching elevation...")
    }
    
    elev_data_sets <- lookup_elevation(elev_data_sets, get_slope, get_aspect)
    
    for(i in 1:length(data_info$data)) {
      
      aniid <- data_info$ani[i]
      
      #on every 50th data file, increment file name counter and wipe data_sets
      if(i %% 50 == 0 & i > 49) {
        saveRDS(data_sets, data_info$rds_name)
        num_saved_rds <- num_saved_rds + 1
        data_info$rds_name <- paste0(data_dir, num_saved_rds, ".rds")
        data_sets <- list()
      }
      
      df_out <- elev_data_sets %>% dplyr::filter(Animal == aniid)
      
      # get meta from df
      file_meta <- get_meta(df_out, i, data_info$file[i], data_info$site[i], aniid, data_info$rds_name)
      # save meta to the designated meta df
      meta_df <- save_meta(meta_df, file_meta)
      # replace df with elevation df
      data_sets[[paste0("ani",aniid)]] <- df_out
      incProgress(1/(2*length(data_info$data)), detail = paste0(i,"/",length(data_info$data), " files completed"))
    }
    
  }) #progress bar
  #save remaining data files
  saveRDS(data_sets, rds_name)
  return(meta_df)
}


#'
#'Generate metadata for an animal data frame -
#'filename, site, date min/max, animals, min/max lat/longitude, storage location 
#'
#'@param df clean animal data frame 
#'@param file_id ID number of .csv source of animal data frame
#'@param file_name .csv source of animal data frame
#'@param site physical source of animal data
#'@param ani_id ID of animal found in data frame
#'@param storage_loc .rds storage location of animal data frame
#'@return df of metadata for animal data frame 
get_meta <- function(df, file_id, file_name, site, ani_id, storage_loc) {
   return(data.frame(file_id = file_id, 
            file_name = file_name, 
            site = site, 
            ani_id = ani_id, 
            min_date = min(df$Date), 
            max_date = max(df$Date), 
            min_lat = min(df$Latitude), 
            max_lat = max(df$Latitude),
            min_long = min(df$Longitude), 
            max_long = max(df$Longitude), 
            storage = storage_loc))
}

#'
#'Save metadata to a data frame and return it
#'
#'@param meta_df the data frame to store metadata in
#'@param file_meta meta for a .csv file generated by get_meta
#'
save_meta <- function(meta_df, file_meta) {
  meta_df <- rbind(meta_df, file_meta)
  return(meta_df)
}

#'
#'Get animal data set from specified meta
#'
#'@param meta_df data frame of specified meta
#'@param min_date minimum date specified by user
#'@param max_date maximum date specified by user
#'
get_data_from_meta <- function(meta_df, min_date, max_date) {
  
  meta_df$storage <- as.character(meta_df$storage)
  rds_files <- list(unique(meta_df$storage))
  current_df <- data.frame()
  for(file_name in rds_files) {
    current_rds <- readRDS(file_name)
    for(df in current_rds) {
      current_df <- rbind(current_df, df)
    }
  }
  
  current_df <- current_df %>%
    dplyr::filter(Animal %in% meta_df$ani_id,
                  Date <= max_date,
                  Date >= min_date)
  # print( paste("nrows =", nrow(current_df) ) )
  
  return(current_df)
}

